- type: Schedule
  members:
    - speaker: Mangpo Phothilimthana (Google)
      date: M 10/09/23
      title: "ML for ML Compilers"
      livestream: https://www.youtube.com/watch?v=VASg2XNgj-4
      abstract: "Search-based techniques have been demonstrated effective in solving complex optimization problems that arise in domain-specific compilers for machine learning (ML). Unfortunately, deploying such techniques in production compilers is impeded by several limitations. In this talk, I will present an autotuner for production ML compilers that can tune both graph-level and subgraph-level optimizations at multiple compilation stages. We demonstrate how to incorporate machine learning techniques such as a learned cost model and various learning-based search strategies to reduce autotuning time. Our learned cost model has high accuracy and outperforms a heavily-optimized analytical performance model. In an evaluation across 150 ML training and inference models on Tensor Processing Units (TPUs), the autotuner offers up to 2.4x and an average 5% runtime speedup over the heavily-optimized XLA compiler. I will outline how we deploy the learning-based XLA autotuner at datacenter scale to automatically tune the most heavily-used production models in Google's fleet everyday. The deployed tile size autotuner has been saving approximately 2% of fleetwide TPU compute time. We recently released a public dataset (https://github.com/google-research-datasets/tpu_graphs) for the learned cost model, and host an on-going Kaggle competition on the dataset (https://www.kaggle.com/competitions/predict-ai-model-runtime) to promote more research in ML for Systems."
      bio: "Phitchaya Mangpo Phothilimthana is a Staff research scientist at Google DeepMind (previously Google Brain), where she leads Machine Learning for Machine Learning Compilers effort (one of Google Brain moonshots in 2020). Her research interests include compilers, machine learning for systems, program synthesis, and energy-aware computing. Mangpo received an undergraduate degree in Computer Science from MIT and PhD from UC Berkeley. Mangpo was a recipient of Microsoft Research PhD Fellowship and Qualcomm Innovation Fellowship."
    - speaker: Martin Maas (Google)
      date: M 10/16/23
      title: "A Taxonomy of Machine Learning for Systems Problems"
      abstract: "Machine learning has the potential to significantly improve computer systems. While recent research in this area has shown great promise, not all problems are equally well-suited for applying ML techniques, and some remaining challenges have prevented wider adoption of ML in systems. In this talk, I will introduce a taxonomy to classify machine learning for systems approaches, discuss how to identify cases that are a good fit for machine learning, and lay out a longer-term vision of how different systems can be improved using ML techniques, ranging from computer architecture to language runtimes."
      bio: "Martin Maas is a Staff Research Scientist at Google DeepMind. His research interests are in language runtimes, computer architecture, systems, and machine learning, with a focus on applying machine learning to systems problems. Before joining Google, Martin completed his PhD in Computer Science at the University of California at Berkeley, where he worked on hardware support for managed languages and architectural support for memory-trace obliviousness."
      livestream: https://www.youtube.com/watch?v=4_UdAR_5jqk
    - speaker: Tim Dettmers (UW)
      date: M 10/23/23
      title: "Democratizing Foundation Models via k-bit Quantization"
      abstract: "Foundation models are effective tools for many tasks but are challenging to finetune and inference due to their GPU memory requirements. Compressing foundation models with k-bit quantization makes them more accessible with minimal resources, but k-bit quantization can lead to degradation in model quality. In this lecture, I will talk about fundamental insights into how to compress foundation models with quantization while maintaining their predictive performance. We will learn about emergent outliers in large language models (LLMs) and how they affect performance during 8-bit quantization. We will learn how to do effective k-bit compression of pretrained large language models such that we maximize their density of predictive performance per bit. We will also talk about how to do efficient fine-tuning of quantized 4-bit LLMs (QLoRA) and how this helps to build state-of-the-art chatbots."
      bio: "Tim Dettmers is a graduating PhD student advised by Luke Zettlemoyer at the University of Washington in Seattle. He holds degrees in applied math and computer science and has a background in industrial automation. His primary research goal is to democratize foundation models by making them more efficient and accessible through quantization, sparsification, and building machine learning systems that use consumer-grade hardware. He is the creator of the bitsandbytes library. Tim runs a blog about deep learning, GPUs, and PhD life at timdettmers.com."
      livestream: https://www.youtube.com/watch?v=EsMcVkTXZrk
    - speaker: Deepak Narayanan (Nvidia)
      date: M 10/30/23
      title: "Training Large Language Models at Scale"
      abstract: "Training LLMs efficiently is challenging for a few reasons: training can require yottaFLOPs of compute,  and accelerators have limited memory capacity making it impossible to fit large models on even a multi-GPU server. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, na√Øve usage of these methods leads to scaling issues at thousands of GPUs. In this talk, I describe various systems innovations incorporated into Megatron-LM (https://github.com/nvidia/megatron-lm) that allow us to run training iterations for models with up to a trillion parameters on thousands of GPUs."
      bio: "Deepak is a Senior Applied Deep Learning Research Scientist in the ADLR group at NVIDIA, where he builds software systems to more efficiently train and serve LLMs. He graduated from Stanford with a Ph.D. in Computer Science in September 2021, where he was advised by Prof. Matei Zaharia."
      livestream: https://www.youtube.com/watch?v=JA1l96tjrs4
    - speaker: Travis Addair (Predibase)
      date: M 11/06/23
      title: "Serving 100s of Fine-Tuned LLMs on 1 GPU with LoRAX"
      abstract: "Smaller, specialized language models such as LLaMA-2-7b can outperform larger general-purpose models like GPT-4 when fine-tuned on proprietary data to perform a single task. But serving many fine-tuned LLMs in production can quickly add up to tens of thousands of dollars per month in cloud costs when each model requires its own dedicated GPU resources. LoRA Exchange (LoRAX) is an LLM inference system built for serving numerous fine-tuned LLMs using a shared set of GPU resources. With LoRAX, users can pack over 100 task-specific models into a single GPU, significantly reducing the expenses associated with serving fine-tuned models by orders of magnitude over dedicated deployments. In this seminar, we'll explore the challenges of serving fine-tuned LLMs in production, and the motivation behind building a system like LoRAX. We'll introduce parameter efficient fine-tuning adapters like Low Rank Adaptation (LoRA), and show how LoRAX dynamically loads and exchanges different adapters at runtime, leveraging a tiered weight cache to speed up this exchange process. Additionally, we'll show how LoRAX achieves high throughput with continuous multi-adapter batching, allowing requests from different fine-tuned adapters to batch together within a single decoding step."
      bio: "Travis Addair is co-founder and CTO of Predibase, the AI platform for engineers. Within the Linux Foundation, he serves as lead maintainer for the Horovod distributed deep learning framework and is a co-maintainer of the Ludwig automated deep learning framework. In the past, he led Uber's deep learning training team as part of the Michelangelo machine learning platform."
      livestream: https://www.youtube.com/watch?v=i6zVvfvIFpc
    - speaker: William Fedus (OpenAI)
      date: M 11/13/23
    - speaker: Thanksgiving
    - speaker: Tianqi Chen (CMU)
      date: M 11/27/23
    - speaker: Dan Fu (Stanford, Together)
      date: M 12/04/23